\subchapter
{System wide profiling and tracing}
{Objectives:
  \begin{itemize}
    \item IRQ latencies using {\em ftrace}.
    \item Tracing and visualizing system activity using {\em kernelshark} or
          {\em LTTng}
    \item System profiling with {\em perf}.
  \end{itemize}
}

\section{IRQ latencies using {\em ftrace}}

{\em ftrace} features a specific tracer for irq latency which is named irqsoff.
Using this tracer, analyze the system irqs latency. Run the following command
for a few seconds and hit \code{Ctrl + [C]} to stop it.

\begin{bashinput}
$ trace-cmd record -p irqsoff
^C
\end{bashinput}

Once done, you can visualize which section of code generated too much latency by
using:

\begin{bashinput}
$ trace-cmd report
\end{bashinput}

This will display a trace of the longest chain of calls while interrupts were
disabled. Based on this report, can you find the code that generates this
latency ?

\section{ftrace \& uprobes}

First of all, we will start a small program using the following command:

\begin{bashinput}
$ mystery_program 1000 200 2 &
\end{bashinput}

In order to trace a full system, we can use ftrace. However, if we want to trace
the userspace, we'll need to add new tracepoints using uprobes. This can be done
manually with the uprobe sysfs interface or using \code{perf probe}.

Before starting to profile, we will compile our program to be instrumented:

\begin{bashinput}
$ cd /home/<user>/debugging-labs/nfsroot/root/system_profiling
$ make
\end{bashinput}

On the target, we will create a uprobe in the main function of the
\code{crc_random} program each time a crc is computed. First, let list the lines
number that are recognized by perf to add a uprobe:

\begin{bashinput}
$ perf probe --source=./ -x ./crc_random -L main
\end{bashinput}

\em Note: in order to be able to add such userspace probe, perf needs to access
symbols and source file

Then, we can create a uprobe and capture the crc value using:

\begin{bashinput}
$ perf probe --source=./ -x ./crc_random main:35 crc
\end{bashinput}

We can finally trace the application using trace-cmd with this event

\begin{bashinput}
$ trace-cmd record -e probe_crc_random:main_L35 ./crc_random
^C
\end{bashinput}

Then, using kernelshark tool on the host, analyze the trace:

\begin{bashinput}
$ sudo apt install kernelshark
$ kernelshark
\end{bashinput}

We can see that something is wrong, our process does not seems to compute crc at
a fixed rate. Let's trace the \code{sched_switch} events to see whats happening:

\begin{bashinput}
$ trace-cmd record -e sched_switch -e probe_crc_random:main_L35 ./crc_random
^C
\end{bashinput}

Reanalyze the traces with kernel shark and try to understand what is going on.

\section{LTTng}

In order to observe our program performance, we want to instrument it with
tracepoints. We would like to know how much times it takes to compute the
crc32 of a specific buffer.

In order to do so, add tracepoints to your program which will allows to measure
this. We'll add 2 tracepoints:

\begin{itemize}
  \item One for the start of crc32 computation (\code{compute_crc_start})
    without any arguments.
  \item Another for the end of crc32 computation (\code{compute_crc_end}) with
      a crc \code{int} argument that will be displayed as an hexadecimal integer
      using \code{lttng_ust_field_integer_hex()} output formatter.
\end{itemize}

In order to create these tracepoints easily, we will create a
\code{crc_random-tp.tp} file and generate the tracepoints using
\code{lttng-gen-tp}. These tracepoints should belong to the \code{crc_random}
provider namespace. Once written, generate the tracepoints provider package
header and template using:

Once added, you can modify the \code{Makefile} by adding a new rule and
modifying the existing one to use the generated files.

\begin{bashinput}
  crc_random-tp.o: crc_random-tp.tp
    lttng-gen-tp $<
  
  crc_random: crc_random.c crc_random-tp.o
    ${CC} $^ -g3 -I. -llttng-ust -o $@
  \end{bashinput}

You can then use the new tracepoints in your program to trace specific points
of execution as requested.

Finally, on the target, enable the program tracepoints, run it and collect
tracepoints. We are going to do that remotely using the \code{lttng-relayd} tool
on the remote computer:

\begin{bashinput}
$ sudo apt install lttng-tools
$ lttng-relayd --output=$PWD/traces
\end{bashinput}

Then on the target, start the trace acquisition using:

\begin{bashinput}
$ lttng-sessiond --daemonize
$ lttng create crc_session --set-url=net://192.168.0.1
$ lttng enable-event --userspace crc_random:compute_crc_start
$ lttng enable-event --userspace crc_random:compute_crc_end
$ lttng enable-event --kernel sched_switch
$ lttng start
$ ./crc_random
$ lttng destroy
\end{bashinput}

Once finished, the traces will be visible in \code{$PWD/traces/<hostname>/<session>}
on the remote computer. In our case, the hostname is buildroot so traces will be
located in \code{$PWD/traces/buildroot/<session>}

Using \code{babeltrace2}, you can display the raw traces that were acquired:
\begin{bashinput}
$ babeltrace2 $PWD/traces/buildroot/<session>/
\end{bashinput}

In order to analyze our traces more visually, we are going to use tracecompass.
Download \code{tracecompass} latest version and extract it using:

\begin{bashinput}
$ wget https://ftp.fau.de/eclipse/tracecompass/releases/8.1.0/rcp/trace-compass-8.1.0-20220919-0815-linux.gtk.x86_64.tar.gz
$ tar -xvf trace-compass-*.tar.gz
\end{bashinput}

Run it
\begin{bashinput}
$ cd trace-compass*
$ ./tracecompass
\end{bashinput}

We are going to merge the kernel and the user traces in tracecompass. To do so,
open the traces using the \code{File -> Open Trace...} menu and open the traces
by loading both the \code{kernel} (\code{kernel/channel0_0}) and the \code{ust}
(\code{ust/uid/0/32-bit/channel0_0}) folders.

Once opened, in the left pane, expand the \code{Tracing} item and right click
on \code{Experiments[0]}. Select \code{New}, name the new experiment
\code{debugging_lab}. Then expand the \code{Experiments[1]} item and right click on
the \code{debugging_lab} one and click on \code{Select traces...}, In the new
window, check the 32-bit box and the kernel one, then click on \code{Finish}.
Finally, double click on the \code{debugging_lab[2]} item to display the merged
trace. Explore the interface, and try to follow the task execution on both the
Resources view and in the Control flow one.

\section{System profiling with {\em perf}}

In order to profile the whole system, we are going to use perf and try to find
the function that takes most of the time executing.

First of all, we will run a global recording of functions and their backtrace on
(all CPUs) during 10 seconds using the following command:

\begin{bashinput}
$ perf record -F 99 -g -- sleep 10
\end{bashinput}

This command will generate a \code{perf.data} file that can be used on a remote
computer for further analysis. Copy that file and fix the permissions using
\code{chown}:

\begin{bashinput}
$ sudo cp /home/<user>/debugging-labs/nfsroot/root/system_profiling/perf.data .
$ sudo chown <user>:<user> perf.data
\end{bashinput}

We will then use perf report to visualize the aquired data:

\begin{bashinput}
$ perf report --symfs=/home/<user>/debugging-labs/buildroot/output/staging/
  -k /home/<user>/debugging-labs/buildroot/output/build/linux-5.13/vmlinux
\end{bashinput}

Another useful tool for performance analysis is flamegraphs. Latest perf
version includes a builtin support for flamegraphs but the template is not
available on debian so we will use another support provided by Brendan Gregg
scripts.

\begin{bashinput}
$ git clone https://github.com/brendangregg/FlameGraph.git
$ perf script | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl > flame.html
\end{bashinput}

Using this flamegraph, analyze the system load.
