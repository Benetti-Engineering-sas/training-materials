\subchapter
{System wide profiling and tracing}
{Objectives:
  \begin{itemize}
    \item IRQ latencies using {\em ftrace}.
    \item Tracing and visualizing system activity using {\em kernelshark} or
          {\em LTTng}
    \item System profiling with {\em perf}.
  \end{itemize}
}

\section{IRQ latencies using {\em ftrace}}

{\em ftrace} features a specific tracers for irq latency which is named irqsoff.
Using this tracer, analyze the system irqs latency. Run the following command
for a few seconds and hit \code{Ctrl + [C]} to stop it.

\begin{bashinput}
$ trace-cmd record -p irqsoff
^C
\end{bashinput}

Once done, you can visualize which section of code generated too much latency by
using:

\begin{bashinput}
$ trace-cmd report
\end{bashinput}

This will display a trace of the longest chain of calls while itnerrupts were
disabled. Based on this report, can you find the code that generates this
latency ?

\section{LTTng}

Go into the \code{crc_random} directory and open the \code{crc_random.c} file. This file
computes crc32 on a buffer of random data. Compile it using the provided toolchain

\begin{bashinput}
$ cd /home/<user>/debugging-labs/nfsroot/root/system_profiling
$  $(CROSS_COMPILE)-gcc -Wall -Werror crc_random.c -o crc_random
\end{bashinput}

In order to observe our program performances, we want to instrument it with
tracepoints. We would like to know how much times it takes to compute the
crc32 of a specific buffer.

In order to do so, add tracepoints to your program which will allows to measure
this. We'll add 2 tracepoints:

\begin{itemize}
  \item One for the start of crc32 computation (\code{compute_crc_entry})
  \item Another for the end of crc32 computation (\code{compute_crc_exit})
\end{itemize}

For that, create a tracepoint provider header file template named
\code{crc_random-tp.h} and another one for the tracepoint provider named
\code{crc_random-tp.c}. These tracepoints should belong to the \code{crc_random}
provider namespace.

You can then use the new tracepoints in your program to trace specific points
of execution. Once added, you can compile your application using the following
command:

\begin{bashinput}
$ $(CROSS_COMPILE)-gcc -I. crc_random-tp.c crc_random.c  -llttng-ust -o crc_random
\end{bashinput}

Finally, on the target, enable the program tracepoints, run it and collect
tracepoints.

\begin{bashinput}
$ lttng-sessiond --daemonize
$ lttng create crc_session
$ lttng enable-event --userspace crc_random:compute_crc_entry
$ lttng enable-event --userspace crc_random:compute_crc_exit
$ lttng enable-event --kernel sched_switch
$ lttng start
$ ./crc_random
$ lttng destroy
\end{bashinput}

Using \code{babeltrace2}, analyze the traces and see the time that is spent
between the tracepoints.

In order to analyze our traces, we are going to use tracecompass. Download
\code{tracecompass} latest version and extract it using:

\begin{bashinput}
$ wget https://ftp.fau.de/eclipse/tracecompass/releases/8.1.0/rcp/trace-compass-8.1.0-20220919-0815-linux.gtk.x86_64.tar.gz
$ tar -xvf trace-compass-*.tar.gz
\end{bashinput}

Run it
\begin{bashinput}
$ cd trace-compass*
$ ./tracecompass
\end{bashinput}

\section{System profiling with {\em perf}}

In order to profile the whole system, we are going to use perf and try to find
the function that takes most of the time executing.

First of all, we will run a global recording of functions and their backtrace on
(all CPUs) during 10 seconds using the following command:

\begin{bashinput}
$ perf record -F 99 -g -- sleep 10
\end{bashinput}

This command will generate a \code{perf.data} file that can be used on a remote
computer for further analysis. Copy that file and fix the permissions using
\code{chown}:

\begin{bashinput}
$ sudo cp /home/<user>/debugging-labs/nfsroot/root/system_profiling/perf.data .
$ sudo chown <user>:<user> perf.data
\end{bashinput}

We will then use perf report to visualize the aquired data:

\begin{bashinput}
$ perf report -k /home/<user>/debugging-labs/nfsroot/root/vmlinux
\end{bashinput}


Another useful tool for performance analysis is the Flamegraphs. latest perf
version includes a builtin support for flamegraphs but the template is not
available on debian so we will use another support provided by Brendan Gregg
scripts.

\begin{bashinput}
$ git clone https://github.com/brendangregg/FlameGraph.git
$ perf script | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl > flame.html
\end{bashinput}

Using this flamegraph, analyze the system load.