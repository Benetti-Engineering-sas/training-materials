\subchapter
{System wide profiling and tracing}
{Objectives:
  \begin{itemize}
    \item IRQ latencies using {\em ftrace}.
    \item Tracing and visualizing system activity using {\em kernelshark} or
          {\em LTTng}
    \item System profiling with {\em perf}.
  \end{itemize}
}

\section{IRQ latencies using {\em ftrace}}

{\em ftrace} features a specific tracer for irq latency which is named irqsoff.
Using this tracer, analyze the system irqs latency. Run the following command
for a few seconds and hit \code{Ctrl + [C]} to stop it.

\begin{bashinput}
$ trace-cmd record -p irqsoff
^C
\end{bashinput}

Once done, you can visualize which section of code generated too much latency by
using:

\begin{bashinput}
$ trace-cmd report
\end{bashinput}

This will display a trace of the longest chain of calls while interrupts were
disabled. Based on this report, can you find the code that generates this
latency ?

\section{ftrace \& uprobes}

First of all, we will start a small program using the following command:

\begin{bashinput}
$ mystery_program 1000 200 2 &
\end{bashinput}

In order to trace a full system, we can use ftrace. However, if we want to trace
the userspace, we'll need to add new tracepoints using uprobes. This can be done
manually with the uprobe sysfs interface or using \code{perf probe}.

Before starting to profile, we will compile our program to be instrumented:

\begin{bashinput}
$ cd /home/<user>/debugging-labs/nfsroot/root/system_profiling
$  $(CROSS_COMPILE)-gcc -Wall -Werror -g3 crc_random.c -o crc_random
\end{bashinput}

On the target, we will create a uprobe in the main function of the
\code{crc_random} program each time a crc is computed. First, let list the lines
number that are recognized by perf to add a uprobe:

\begin{bashinput}
$ perf probe --source=./ -x ./crc_random -L main
\end{bashinput}

\em Note: in order to be able to add such userspace probe, perf needs to access
symbols and source file

Then, we can create a uprobe and capture the crc value using:

\begin{bashinput}
$ perf probe --source=./ -x ./crc_random main:35 crc
\end{bashinput}

We can finally trace the application using trace-cmd with this event

\begin{bashinput}
$ trace-cmd record -e probe_crc_random:main_L35 ./crc_random
^C
\end{bashinput}

Then, using kernelshark tool on the host, analyze the trace:

\begin{bashinput}
$ kernelshark
\end{bashinput}

We can see that something is wrong, our process does not seems to compute crc at
a fixed rate. Let's trace the \code{sched_switch} events to see whats happening:

\begin{bashinput}
$ trace-cmd record -e probe_crc_random:main_L35 ./crc_random
^C
\end{bashinput}

Reanalyze the traces with kernel shark and try to understand what is going on.

\section{LTTng}

In order to observe our program performances, we want to instrument it with
tracepoints. We would like to know how much times it takes to compute the
crc32 of a specific buffer.

In order to do so, add tracepoints to your program which will allows to measure
this. We'll add 2 tracepoints:

\begin{itemize}
  \item One for the start of crc32 computation (\code{compute_crc_entry})
  \item Another for the end of crc32 computation (\code{compute_crc_exit})
\end{itemize}

For that, create a tracepoint provider header file template named
\code{crc_random-tp.h} and another one for the tracepoint provider named
\code{crc_random-tp.c}. These tracepoints should belong to the \code{crc_random}
provider namespace.

You can then use the new tracepoints in your program to trace specific points
of execution. Once added, you can compile your application using the following
command:

\begin{bashinput}
$ $(CROSS_COMPILE)-gcc -I. crc_random-tp.c crc_random.c  -llttng-ust -o crc_random
\end{bashinput}

Finally, on the target, enable the program tracepoints, run it and collect
tracepoints.

\begin{bashinput}
$ lttng-sessiond --daemonize
$ lttng create crc_session
$ lttng enable-event --userspace crc_random:compute_crc_entry
$ lttng enable-event --userspace crc_random:compute_crc_exit
$ lttng enable-event --kernel sched_switch
$ lttng start
$ ./crc_random
$ lttng destroy
\end{bashinput}

Using \code{babeltrace2}, analyze the traces and see the time that is spent
between the tracepoints.

In order to analyze our traces, we are going to use tracecompass. Download
\code{tracecompass} latest version and extract it using:

\begin{bashinput}
$ wget https://ftp.fau.de/eclipse/tracecompass/releases/8.1.0/rcp/trace-compass-8.1.0-20220919-0815-linux.gtk.x86_64.tar.gz
$ tar -xvf trace-compass-*.tar.gz
\end{bashinput}

Run it
\begin{bashinput}
$ cd trace-compass*
$ ./tracecompass
\end{bashinput}

\section{System profiling with {\em perf}}

In order to profile the whole system, we are going to use perf and try to find
the function that takes most of the time executing.

First of all, we will run a global recording of functions and their backtrace on
(all CPUs) during 10 seconds using the following command:

\begin{bashinput}
$ perf record -F 99 -g -- sleep 10
\end{bashinput}

This command will generate a \code{perf.data} file that can be used on a remote
computer for further analysis. Copy that file and fix the permissions using
\code{chown}:

\begin{bashinput}
$ sudo cp /home/<user>/debugging-labs/nfsroot/root/system_profiling/perf.data .
$ sudo chown <user>:<user> perf.data
\end{bashinput}

We will then use perf report to visualize the aquired data:

\begin{bashinput}
$ perf report -k /home/<user>/debugging-labs/nfsroot/root/vmlinux
\end{bashinput}


Another useful tool for performance analysis is flamegraphs. Latest perf
version includes a builtin support for flamegraphs but the template is not
available on debian so we will use another support provided by Brendan Gregg
scripts.

\begin{bashinput}
$ git clone https://github.com/brendangregg/FlameGraph.git
$ perf script | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl > flame.html
\end{bashinput}

Using this flamegraph, analyze the system load.